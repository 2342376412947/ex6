# <font face="黑体" size=12><center>实验报告：基于 Inception-v3 的 Fashion-MNIST 迁移学习</center></font>

### 一、实验目的

掌握深度卷积神经网络 Inception-v3 的模型结构特点及其对输入数据的具体要求

学习迁移学习Transfer Learning的核心思想，利用 ImageNet 预训练权重加速模型收敛

实现对单通道小尺寸图像Fashion-MNIST的处理，使其适配复杂大模型，并完成模型的微调、训练与测试流程

### 二、实验原理

1.用Inception-v3 模型来提取特征，Inception-v3是一种深度卷积神经网络，核心在于通过不同尺寸的卷积核并行处理提取特征。ps：由于模型结构设计，Inception-v3 要求输入图像尺寸为 $299 \times 299$，且通常需要 3 通道输入

2.本实验加载了在 ImageNet 大型数据集上预训练的权重Inception_V3_Weights.DEFAULT，保留了模型中的卷积层参数，替换最后的全连接层FC Layer来适配 Fashion-MNIST 的 10 分类任务

3.至于数据预处理，Fashion-MNIST 原始数据为 28*28 的单通道灰度图，为了适配 Inception-v3，需要将其上采样至 299*299 ，并将灰度通道复制扩展为 3 通道

### 三、实验环境

操作系统：Windows 软件环境：Python 3.13.5、PyTorch、torchvision、Numpy、Jupyter Notebook 硬件环境：CUDA (GPU) 

### 四、实验步骤

#### 1. 数据集准备与预处理加载 Fashion-MNIST 数据集

设置n = 100，也就是仅随机抽取原数据集的 1 / 100 进行实验。此外，关键预处理还包括将图像放大至 299 像素并转为 3 通道：

```python
#%%
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models
import torch
from torch.utils.data import Subset
from torchsummary import summary
import numpy as np

batch_size = 16
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#----1.数据:Fashion-MNIST 3x299x299,仅取n%----
transform = transforms.Compose([
    transforms.Resize(299), # 缩放图像到299x299（Inception-v3要求的输入尺寸）
    transforms.Grayscale(num_output_channels=3),# 灰度图转3通道（Inception-v3要求3通道输入）
    transforms.ToTensor()  # 转换为Tensor：将PIL图像转为[0,1]范围的张量，维度为(C,H,W)
])
train_full = datasets.FashionMNIST('data', train=True, download=True, transform=transform)
test_full = datasets.FashionMNIST('data', train=False, download=True, transform=transform)

n = 100  # 根据硬件实际情况选取不同比例的数据
rng = np.random.default_rng(42) # 固定随机种子42
train_idx = rng.choice(len(train_full), len(train_full) // n, replace=False)
test_idx = rng.choice(len(test_full), len(test_full) // n, replace=False)

# 创建数据加载器
train_loader = torch.utils.data.DataLoader(Subset(train_full, train_idx), batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(Subset(test_full, test_idx), batch_size=batch_size, shuffle=False)

```

#### 2. 模型构建与迁移学习设置

加载预训练的 Inception-v3 模型，替换全连接层，并定义优化器

```python
#----2.导入官方Inception-v3和权重,只换fc----
model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)
model.fc = nn.Linear(model.fc.in_features, 10)# 替换全连接层
model = model.to(device)

# 定义优化器：Adam优化器，学习率1e-4（微调预训练模型用小学习率），weight_decay=1e-5（L2正则化，防止过拟合）
optimizer = optim.Adam(model.parameters(), lr=1e-4)
# summary(model, input_size=(3, 299, 299))   #打印模型
```

#### 3. 模型训练与测试
   
执行 10 轮训练，Inception-v3返回一个包含主输出和辅助输出的元组InceptionOutputs，所以代码中要用out = model(x).logits 来获取主分类器的输出用于计算损失

```python
#----3.训练/测试----
accs, losses = [], []
epochs = 10
for epoch in range(epochs):
    model.train()
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        
        optimizer.zero_grad()
        out = model(x).logits
        
        loss = F.cross_entropy(out, y)
        loss.backward()
        optimizer.step()
    
    model.eval()
    with torch.no_grad():
        correct, total_loss = 0, 0.
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            total_loss += F.cross_entropy(out, y).item()
            correct += (out.argmax(1) == y).sum().item()
    
    acc = correct / len(test_loader.dataset)
    avg_loss = total_loss / len(test_loader)
    accs.append(acc)
    losses.append(avg_loss)
    
    print(f'epoch {epoch}: loss={avg_loss:.4f}, acc={acc:.4f}')
#%%
```

### 五、实验结果与分析

训练过程中的测试集 Loss 与 Accuracy 变化如下表所示：
| 训练轮数（epoch） | 测试损失（loss） | 测试准确率（acc） |
| :--- | :--- | :--- |
| 0 | 1.0314 | 0.6600 |
| 1 | 0.5821 | 0.7700 |
| 2 | 0.5644 | 0.7800 |
| 3 | 0.5485 | 0.7900 |
| 4 | 0.5381 | 0.7800 |
| 5 | 0.5444 | 0.7800 |
| 6 | 0.5964 | 0.7600 |
| 7 | 0.5293 | 0.8000 |
| 8 | 0.7567 | 0.7300 |
| 9 | 0.5786 | 0.7800 |

#### 结果分析：

因为有 ImageNet 预训练权重，模型收敛很快，首轮准确率即达 77%。随后准确率维持在 78%-80% 区间，但为了方便训练演示，n只设置为100，训练样本仅占原数据集的 1%，导致模型泛化稳定性不足，后期出现了 Loss 震荡（如 Epoch 8）。总体而言，实验成功验证了 Inception-v3 在极小样本下对灰度图像进行迁移学习的有效性

### 六、实验总结
本次实验成功实现了基于 Inception-v3 的 Fashion-MNIST 图像分类，通过 torchvision.transforms 有效解决了原始数据与模型输入（299 * 299, 3通道）不匹配的问题。此外，实验还验证了迁移学习在小样本场景下的有效性，用少量的训练数据（1%）即可快速达到 80% 左右的准确率。同时，实验中也注意到了 Inception 模型在 train() 模式下特殊的输出结构（logits/aux_logits），对我的帮助十分大

